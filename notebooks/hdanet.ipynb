{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5af16b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ASPP(nn.Module):\n",
    "    \"\"\"Atrous Spatial Pyramid Pooling (ASPP) module.\"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ASPP, self).__init__()\n",
    "        dilations = [1, 6, 12, 18]\n",
    "        \n",
    "        self.aspp1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels), nn.ReLU())\n",
    "        self.aspp2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=dilations[1], dilation=dilations[1], bias=False),\n",
    "            nn.BatchNorm2d(out_channels), nn.ReLU())\n",
    "        self.aspp3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=dilations[2], dilation=dilations[2], bias=False),\n",
    "            nn.BatchNorm2d(out_channels), nn.ReLU())\n",
    "        self.aspp4 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=dilations[3], dilation=dilations[3], bias=False),\n",
    "            nn.BatchNorm2d(out_channels), nn.ReLU())\n",
    "\n",
    "        self.global_avg_pool = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            nn.Conv2d(in_channels, out_channels, 1, stride=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels), nn.ReLU())\n",
    "\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(out_channels * 5, out_channels, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels), nn.ReLU())\n",
    "        # Dropout layer added\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.aspp1(x)\n",
    "        x2 = self.aspp2(x)\n",
    "        x3 = self.aspp3(x)\n",
    "        x4 = self.aspp4(x)\n",
    "        x5 = self.global_avg_pool(x)\n",
    "        x5 = F.interpolate(x5, size=x4.size()[2:], mode='bilinear', align_corners=True)\n",
    "        \n",
    "        x = torch.cat((x1, x2, x3, x4, x5), dim=1)\n",
    "        x = self.conv1(x)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf84ba10",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DifferenceAttentionModule(nn.Module):\n",
    "    \"\"\"\n",
    "    The 'Difference attention' block from the diagram.\n",
    "    It learns a mask to apply to the absolute difference.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels):\n",
    "        super(DifferenceAttentionModule, self).__init__()\n",
    "        # A simple gate to learn the attention mask\n",
    "        self.attention_gate = nn.Sequential(\n",
    "            nn.Conv2d(in_channels * 2, in_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(in_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels, 1, kernel_size=1, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, f_t1, f_t2):\n",
    "        f_concat = torch.cat([f_t1, f_t2], dim=1)\n",
    "        f_diff = torch.abs(f_t1 - f_t2)\n",
    "        \n",
    "        attention_mask = self.attention_gate(f_concat)\n",
    "        \n",
    "        # Refine the difference by multiplying it with the learned mask\n",
    "        return f_diff * attention_mask\n",
    "\n",
    "\n",
    "class HDANet_Head(nn.Module):\n",
    "    \"\"\"\n",
    "    The full head of the HDANet, which corresponds to the \n",
    "    'ASPP', 'Difference attention', and 'Upsample' blocks in the diagram.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels_list, out_channels=256, num_classes=1):\n",
    "        super(HDANet_Head, self).__init__()\n",
    "        \n",
    "        # Create ASPP and DAM for each feature level\n",
    "        self.aspp_modules = nn.ModuleList()\n",
    "        self.dam_modules = nn.ModuleList()\n",
    "        \n",
    "        for in_c in in_channels_list:\n",
    "            # As per diagram, ASPP is applied to each branch's features\n",
    "            # We'll have ASPP output a consistent 'out_channels'\n",
    "            self.aspp_modules.append(ASPP(in_c, out_channels))\n",
    "            # The DAM will then process these 256-channel features\n",
    "            self.dam_modules.append(DifferenceAttentionModule(out_channels))\n",
    "            \n",
    "        # Final fusion and upsampling layers\n",
    "        self.fuse_conv = nn.Sequential(\n",
    "            nn.Conv2d(len(in_channels_list) * out_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.final_classifier = nn.Sequential(\n",
    "            nn.Conv2d(out_channels, 128, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, num_classes, kernel_size=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, t1_features, t2_features):\n",
    "        # t1_features and t2_features are lists of tensors from the HRNet backbone\n",
    "        \n",
    "        refined_diff_maps = []\n",
    "        # Target size is the largest feature map (the first one)\n",
    "        target_size = t1_features[0].shape[2:]\n",
    "        \n",
    "        for i in range(len(t1_features)):\n",
    "            f_t1 = self.aspp_modules[i](t1_features[i])\n",
    "            f_t2 = self.aspp_modules[i](t2_features[i]) # Using same ASPP module\n",
    "            \n",
    "            diff_map = self.dam_modules[i](f_t1, f_t2)\n",
    "            \n",
    "            # Upsample all difference maps to the same size for concatenation\n",
    "            diff_map_upsampled = F.interpolate(diff_map, size=target_size, mode='bilinear', align_corners=False)\n",
    "            refined_diff_maps.append(diff_map_upsampled)\n",
    "        \n",
    "        # Concatenate all refined, upsampled maps\n",
    "        fused_diff = torch.cat(refined_diff_maps, dim=1)\n",
    "        \n",
    "        fused_diff = self.fuse_conv(fused_diff)\n",
    "        \n",
    "        # Final upsample to original image size (HRNet's first stage is 1/4)\n",
    "        out = F.interpolate(fused_diff, scale_factor=4, mode='bilinear', align_corners=False)\n",
    "        out = self.final_classifier(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fea62c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "\n",
    "class HDANet(nn.Module):\n",
    "    def __init__(self, n_channels=3, n_classes=1, pretrained=True):\n",
    "        super(HDANet, self).__init__()\n",
    "        \n",
    "        # 1. Load the HRNet backbone\n",
    "        # We'll use hrnet_w18 as an example.\n",
    "        # `features_only=True` makes it return a list of feature maps\n",
    "        # from each stage, just like the diagram.\n",
    "        self.backbone = timm.create_model(\n",
    "            'hrnet_w18',\n",
    "            pretrained=pretrained,\n",
    "            features_only=True,\n",
    "            in_chans=n_channels\n",
    "        )\n",
    "        \n",
    "        # Get the output channel counts from the backbone\n",
    "        # hrnet_w18 outputs: [64, 128, 256, 512] at 1/4, 1/8, 1/16, 1/32 res\n",
    "        # BUT the diagram shows 4 parallel streams: 1/4, 1/8, 1/16, 1/32\n",
    "        # `timm`'s `features_only=True` for HRNet actually returns [18, 36, 72, 144]\n",
    "        # or similar for hrnet_w18. Let's verify.\n",
    "        \n",
    "        # Let's dynamically get the output channels\n",
    "        dummy_input = torch.randn(2, n_channels, 256, 256)\n",
    "        dummy_features = self.backbone(dummy_input)\n",
    "        \n",
    "        # This is a list of channel dims for each feature map\n",
    "        # e.g., for hrnet_w18: [64, 128, 256, 512]\n",
    "        # NOTE: The diagram shows 4 levels. hrnet_w18 has 4 stages.\n",
    "        # Let's use the output of the 4 stages.\n",
    "        # For 'hrnet_w18', timm.create_model returns features with channels:\n",
    "        # [64, 128, 256, 512]\n",
    "        # A true HRNet from the paper has [C, 2C, 4C, 8C] at 4 resolutions\n",
    "        # The timm 'features_only' output is what we want.\n",
    "        \n",
    "        # Let's re-init backbone to get channel numbers\n",
    "        self.backbone = timm.create_model('hrnet_w18', pretrained=pretrained, features_only=True, in_chans=n_channels)\n",
    "        \n",
    "        # Get the feature info\n",
    "        feature_channels = self.backbone.feature_info.channels()\n",
    "        # This will be something like [64, 128, 256, 512]\n",
    "        \n",
    "        # 2. Create the custom HDANet Head\n",
    "        self.head = HDANet_Head(\n",
    "            in_channels_list=feature_channels,\n",
    "            out_channels=256, # Internal processing dim\n",
    "            num_classes=n_classes\n",
    "        )\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        # 1. Pass T1 through the SHARED backbone\n",
    "        # This returns a list of 4 feature maps\n",
    "        t1_features = self.backbone(x1)\n",
    "        \n",
    "        # 2. Pass T2 through the *EXACT SAME* backbone\n",
    "        # This is where weights are shared!\n",
    "        t2_features = self.backbone(x2)\n",
    "        \n",
    "        # 3. Pass both sets of features to our custom head\n",
    "        return self.head(t1_features, t2_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9cb1dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Test the HDANet ---\n",
    "model = HDANet(n_channels=3, n_classes=1, pretrained=True)\n",
    "model.eval() # Set to eval mode\n",
    "\n",
    "# Create two dummy input tensors (batch, channels, height, width)\n",
    "# HRNet expects inputs to be divisible by 32\n",
    "dummy_image_t1 = torch.randn(2, 3, 256, 256)\n",
    "dummy_image_t2 = torch.randn(2, 3, 256, 256)\n",
    "\n",
    "# Pass BOTH images through the model\n",
    "output_map = model(dummy_image_t1, dummy_image_t2)\n",
    "\n",
    "# The output is your \"change map\" logits\n",
    "# The head upsamples it back to the original size\n",
    "print(\"HDANet Output Shape:\", output_map.shape)\n",
    "\n",
    "# HDANet Output Shape: torch.Size([2, 1, 256, 256])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
