{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# HFANet Training Notebook\n",
    "**High-Frequency Attention Network for Building Change Detection**\n",
    "\n",
    "This notebook uses the `segmentation-models-pytorch` (smp) backbone version.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-header",
   "metadata": {},
   "source": [
    "## 1. Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')  # Add root directory to path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Project imports\n",
    "from data.dataset import get_dataloader\n",
    "from models.HFANet.hfanet import HFANet\n",
    "from utils.losses import DiceLoss, FocalLoss, SoftIoULoss\n",
    "from utils.training import train_one_epoch, CombinedLoss\n",
    "from utils.evaluation import validate, evaluate_on_loader\n",
    "\n",
    "print(\"All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config-header",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CONFIGURATION - Modify these parameters as needed\n",
    "# ============================================================\n",
    "\n",
    "CONFIG = {\n",
    "    # Data\n",
    "    'data_dir': '../dataset',      # Path to dataset root\n",
    "    'img_size': 256,               # Input image size\n",
    "    'batch_size': 8,               # Batch size\n",
    "    'num_workers': 4,              # DataLoader workers\n",
    "    \n",
    "    # Model\n",
    "    'model_name': 'hfanet',        # Model identifier\n",
    "    'backbone': 'resnet34',        # Backbone encoder\n",
    "    'pretrained': 'imagenet',      # Pretrained weights\n",
    "    'classes': 1,                  # Output classes (binary)\n",
    "    \n",
    "    # Training\n",
    "    'epochs': 100,                 # Max epochs\n",
    "    'lr': 1e-4,                    # Learning rate\n",
    "    'weight_decay': 1e-4,          # Weight decay\n",
    "    'patience': 15,                # Early stopping patience\n",
    "    'threshold': 0.5,              # Binarization threshold\n",
    "    \n",
    "    # Loss\n",
    "    'loss': 'bce+dice',            # Loss function\n",
    "    \n",
    "    # Scheduler\n",
    "    'scheduler': 'plateau',        # LR scheduler type\n",
    "    \n",
    "    # Checkpoints\n",
    "    'checkpoint_dir': '../checkpoints',\n",
    "}\n",
    "\n",
    "# Device setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-header",
   "metadata": {},
   "source": [
    "## 3. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders\n",
    "print(\"Loading datasets...\")\n",
    "\n",
    "train_loader = get_dataloader(\n",
    "    CONFIG['data_dir'],\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    split='train',\n",
    "    img_size=CONFIG['img_size'],\n",
    "    num_workers=CONFIG['num_workers']\n",
    ")\n",
    "\n",
    "val_loader = get_dataloader(\n",
    "    CONFIG['data_dir'],\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    split='val',\n",
    "    img_size=CONFIG['img_size'],\n",
    "    num_workers=CONFIG['num_workers']\n",
    ")\n",
    "\n",
    "print(f\"Train samples: {len(train_loader.dataset)}\")\n",
    "print(f\"Val samples: {len(val_loader.dataset)}\")\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Val batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "visualize-header",
   "metadata": {},
   "source": [
    "## 4. Visualize Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualize-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def denormalize(tensor, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]):\n",
    "    \"\"\"Denormalize image tensor for visualization.\"\"\"\n",
    "    tensor = tensor.clone()\n",
    "    for t, m, s in zip(tensor, mean, std):\n",
    "        t.mul_(s).add_(m)\n",
    "    return tensor.clamp(0, 1)\n",
    "\n",
    "# Get a sample batch\n",
    "sample_batch = next(iter(train_loader))\n",
    "\n",
    "# Visualize first sample\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "\n",
    "img_A = denormalize(sample_batch['image_A'][0]).permute(1, 2, 0).numpy()\n",
    "img_B = denormalize(sample_batch['image_B'][0]).permute(1, 2, 0).numpy()\n",
    "label = sample_batch['label'][0].squeeze().numpy()\n",
    "\n",
    "axes[0].imshow(img_A)\n",
    "axes[0].set_title('Time 1 (Before)')\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(img_B)\n",
    "axes[1].set_title('Time 2 (After)')\n",
    "axes[1].axis('off')\n",
    "\n",
    "axes[2].imshow(label, cmap='gray')\n",
    "axes[2].set_title('Change Mask')\n",
    "axes[2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Image shape: {sample_batch['image_A'].shape}\")\n",
    "print(f\"Label shape: {sample_batch['label'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model-header",
   "metadata": {},
   "source": [
    "## 5. Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model = HFANet(\n",
    "    encoder_name=CONFIG['backbone'],\n",
    "    classes=CONFIG['classes'],\n",
    "    pretrained=CONFIG['pretrained']\n",
    ").to(device)\n",
    "\n",
    "print(f\"Model: HFANet\")\n",
    "print(f\"Backbone: {CONFIG['backbone']}\")\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "test-forward-header",
   "metadata": {},
   "source": [
    "## 6. Test Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-forward",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test forward pass with sample data\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    img_A = sample_batch['image_A'].to(device)\n",
    "    img_B = sample_batch['image_B'].to(device)\n",
    "    \n",
    "    output = model(img_A, img_B)\n",
    "    \n",
    "    print(f\"Input shape: {img_A.shape}\")\n",
    "    print(f\"Output shape: {output.shape}\")\n",
    "    print(f\"Output range: [{output.min().item():.3f}, {output.max().item():.3f}]\")\n",
    "    \n",
    "print(\"\\n✓ Forward pass successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loss-header",
   "metadata": {},
   "source": [
    "## 7. Setup Loss, Optimizer, Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup-training",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function\n",
    "if CONFIG['loss'] == 'bce+dice':\n",
    "    criterion = CombinedLoss([\n",
    "        (nn.BCEWithLogitsLoss(), 1.0),\n",
    "        (DiceLoss(), 1.0),\n",
    "    ])\n",
    "elif CONFIG['loss'] == 'bce+focal':\n",
    "    criterion = CombinedLoss([\n",
    "        (nn.BCEWithLogitsLoss(), 1.0),\n",
    "        (FocalLoss(), 1.0),\n",
    "    ])\n",
    "elif CONFIG['loss'] == 'dice':\n",
    "    criterion = DiceLoss()\n",
    "elif CONFIG['loss'] == 'focal':\n",
    "    criterion = FocalLoss()\n",
    "else:\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "print(f\"Loss function: {CONFIG['loss']}\")\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=CONFIG['lr'],\n",
    "    weight_decay=CONFIG['weight_decay']\n",
    ")\n",
    "print(f\"Optimizer: AdamW (lr={CONFIG['lr']}, weight_decay={CONFIG['weight_decay']})\")\n",
    "\n",
    "# Scheduler\n",
    "if CONFIG['scheduler'] == 'plateau':\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='max', factor=0.5, patience=5, verbose=True\n",
    "    )\n",
    "elif CONFIG['scheduler'] == 'cosine':\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer, T_max=CONFIG['epochs'], eta_min=1e-6\n",
    "    )\n",
    "else:\n",
    "    scheduler = None\n",
    "\n",
    "print(f\"Scheduler: {CONFIG['scheduler']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "training-header",
   "metadata": {},
   "source": [
    "## 8. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "training-loop",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create checkpoint directory\n",
    "os.makedirs(CONFIG['checkpoint_dir'], exist_ok=True)\n",
    "\n",
    "# Training state\n",
    "best_val_iou = 0.0\n",
    "patience_counter = 0\n",
    "history = {\n",
    "    'train_loss': [], 'train_iou': [], 'train_f1': [],\n",
    "    'val_loss': [], 'val_iou': [], 'val_f1': []\n",
    "}\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"STARTING TRAINING\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Epochs: {CONFIG['epochs']}\")\n",
    "print(f\"Early stopping patience: {CONFIG['patience']}\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "for epoch in range(1, CONFIG['epochs'] + 1):\n",
    "    # Train one epoch\n",
    "    train_loss, train_iou, train_f1 = train_one_epoch(\n",
    "        model=model,\n",
    "        dataloader=train_loader,\n",
    "        criterion=criterion,\n",
    "        optimizer=optimizer,\n",
    "        device=device,\n",
    "        epoch=epoch,\n",
    "        threshold=CONFIG['threshold']\n",
    "    )\n",
    "    \n",
    "    # Validate\n",
    "    val_loss, val_iou, val_f1 = validate(\n",
    "        model=model,\n",
    "        dataloader=val_loader,\n",
    "        criterion=criterion,\n",
    "        device=device,\n",
    "        threshold=CONFIG['threshold']\n",
    "    )\n",
    "    \n",
    "    # Save history\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_iou'].append(train_iou)\n",
    "    history['train_f1'].append(train_f1)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_iou'].append(val_iou)\n",
    "    history['val_f1'].append(val_f1)\n",
    "    \n",
    "    # Print epoch summary\n",
    "    print(f\"\\nEpoch [{epoch}/{CONFIG['epochs']}] Summary:\")\n",
    "    print(f\"  Train - Loss: {train_loss:.4f} | IoU: {train_iou:.4f} | F1: {train_f1:.4f}\")\n",
    "    print(f\"  Val   - Loss: {val_loss:.4f} | IoU: {val_iou:.4f} | F1: {val_f1:.4f}\")\n",
    "    \n",
    "    # Update scheduler\n",
    "    if scheduler is not None:\n",
    "        if isinstance(scheduler, optim.lr_scheduler.ReduceLROnPlateau):\n",
    "            scheduler.step(val_iou)\n",
    "        else:\n",
    "            scheduler.step()\n",
    "    \n",
    "    # Save best model\n",
    "    if val_iou > best_val_iou:\n",
    "        best_val_iou = val_iou\n",
    "        patience_counter = 0\n",
    "        \n",
    "        save_path = os.path.join(CONFIG['checkpoint_dir'], f\"best_{CONFIG['model_name']}.pth\")\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_iou': val_iou,\n",
    "            'val_f1': val_f1,\n",
    "            'config': CONFIG,\n",
    "        }, save_path)\n",
    "        print(f\"  ✓ New best model saved! (Val IoU: {best_val_iou:.4f})\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\"  No improvement. Patience: {patience_counter}/{CONFIG['patience']}\")\n",
    "    \n",
    "    # Early stopping\n",
    "    if patience_counter >= CONFIG['patience']:\n",
    "        print(f\"\\nEarly stopping triggered after {epoch} epochs.\")\n",
    "        break\n",
    "    \n",
    "    print()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Best Val IoU: {best_val_iou:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "plot-header",
   "metadata": {},
   "source": [
    "## 9. Plot Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot-history",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "epochs_range = range(1, len(history['train_loss']) + 1)\n",
    "\n",
    "# Loss\n",
    "axes[0].plot(epochs_range, history['train_loss'], label='Train')\n",
    "axes[0].plot(epochs_range, history['val_loss'], label='Val')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# IoU\n",
    "axes[1].plot(epochs_range, history['train_iou'], label='Train')\n",
    "axes[1].plot(epochs_range, history['val_iou'], label='Val')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('IoU')\n",
    "axes[1].set_title('IoU')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# F1\n",
    "axes[2].plot(epochs_range, history['train_f1'], label='Train')\n",
    "axes[2].plot(epochs_range, history['val_f1'], label='Val')\n",
    "axes[2].set_xlabel('Epoch')\n",
    "axes[2].set_ylabel('F1')\n",
    "axes[2].set_title('F1 Score')\n",
    "axes[2].legend()\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(CONFIG['checkpoint_dir'], f\"{CONFIG['model_name']}_training_history.png\"), dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eval-header",
   "metadata": {},
   "source": [
    "## 10. Evaluate on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "evaluate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "best_model_path = os.path.join(CONFIG['checkpoint_dir'], f\"best_{CONFIG['model_name']}.pth\")\n",
    "checkpoint = torch.load(best_model_path, map_location=device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "print(f\"Loaded best model from epoch {checkpoint['epoch']}\")\n",
    "print(f\"Val IoU at save: {checkpoint['val_iou']:.4f}\")\n",
    "\n",
    "# Create test loader\n",
    "test_loader = get_dataloader(\n",
    "    CONFIG['data_dir'],\n",
    "    batch_size=1,\n",
    "    split='test',\n",
    "    img_size=CONFIG['img_size'],\n",
    "    num_workers=CONFIG['num_workers']\n",
    ")\n",
    "print(f\"\\nTest samples: {len(test_loader.dataset)}\")\n",
    "\n",
    "# Evaluate\n",
    "results_dir = os.path.join(CONFIG['checkpoint_dir'], f\"{CONFIG['model_name']}_results\")\n",
    "metrics = evaluate_on_loader(\n",
    "    model=model,\n",
    "    dataloader=test_loader,\n",
    "    device=device,\n",
    "    threshold=CONFIG['threshold'],\n",
    "    save_results_path=os.path.join(results_dir, 'predictions'),\n",
    "    save_pr_curve_path=os.path.join(results_dir, 'pr_curve.png')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "visualize-pred-header",
   "metadata": {},
   "source": [
    "## 11. Visualize Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualize-predictions",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some predictions\n",
    "model.eval()\n",
    "num_samples = 4\n",
    "\n",
    "fig, axes = plt.subplots(num_samples, 4, figsize=(16, 4*num_samples))\n",
    "\n",
    "test_iter = iter(test_loader)\n",
    "for i in range(num_samples):\n",
    "    batch = next(test_iter)\n",
    "    \n",
    "    img_A = batch['image_A'].to(device)\n",
    "    img_B = batch['image_B'].to(device)\n",
    "    label = batch['label']\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(img_A, img_B)\n",
    "        pred = torch.sigmoid(output).cpu()\n",
    "        pred_binary = (pred > CONFIG['threshold']).float()\n",
    "    \n",
    "    # Denormalize images\n",
    "    img_A_vis = denormalize(batch['image_A'][0]).permute(1, 2, 0).numpy()\n",
    "    img_B_vis = denormalize(batch['image_B'][0]).permute(1, 2, 0).numpy()\n",
    "    label_vis = label[0].squeeze().numpy()\n",
    "    pred_vis = pred_binary[0].squeeze().numpy()\n",
    "    \n",
    "    axes[i, 0].imshow(img_A_vis)\n",
    "    axes[i, 0].set_title('Time 1')\n",
    "    axes[i, 0].axis('off')\n",
    "    \n",
    "    axes[i, 1].imshow(img_B_vis)\n",
    "    axes[i, 1].set_title('Time 2')\n",
    "    axes[i, 1].axis('off')\n",
    "    \n",
    "    axes[i, 2].imshow(label_vis, cmap='gray')\n",
    "    axes[i, 2].set_title('Ground Truth')\n",
    "    axes[i, 2].axis('off')\n",
    "    \n",
    "    axes[i, 3].imshow(pred_vis, cmap='gray')\n",
    "    axes[i, 3].set_title('Prediction')\n",
    "    axes[i, 3].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(results_dir, 'sample_predictions.png'), dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary-header",
   "metadata": {},
   "source": [
    "## 12. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"EXPERIMENT SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Model: HFANet (smp)\")\n",
    "print(f\"Backbone: {CONFIG['backbone']}\")\n",
    "print(f\"Loss: {CONFIG['loss']}\")\n",
    "print(f\"Best Val IoU: {best_val_iou:.4f}\")\n",
    "print(f\"\\nTest Results:\")\n",
    "print(f\"  IoU: {metrics['iou']:.4f}\")\n",
    "print(f\"  F1:  {metrics['f1']:.4f}\")\n",
    "print(f\"  Precision: {metrics['precision']:.4f}\")\n",
    "print(f\"  Recall: {metrics['recall']:.4f}\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
