{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "291a224e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/melihkaan/Desktop/CS415PROJECT/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import timm\n",
    "\n",
    "class ResNetFPNFeatureExtractor(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements the Feature Extractor from diagram (b).\n",
    "    Uses a ResNet backbone and FPN-like fusion.\n",
    "    \"\"\"\n",
    "    def __init__(self, backbone_name='resnet18', pretrained=True, \n",
    "                 out_channels=256, in_channels=3):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 1. Load ResNet backbone using timm\n",
    "        self.backbone = timm.create_model(\n",
    "            backbone_name,\n",
    "            pretrained=pretrained,\n",
    "            features_only=True, # Return features from each stage\n",
    "            in_chans=in_channels,\n",
    "            out_indices=(1, 2, 3, 4) # We need stages 1, 2, 3, 4\n",
    "        )\n",
    "        \n",
    "        # Get the channel counts from the backbone stages\n",
    "        backbone_channels = self.backbone.feature_info.channels()\n",
    "        # Example for resnet18: [64, 128, 256, 512]\n",
    "\n",
    "        # 2. Create the 1x1 Convs for channel reduction (lateral layers)\n",
    "        self.lateral_convs = nn.ModuleList()\n",
    "        for in_c in backbone_channels:\n",
    "            self.lateral_convs.append(nn.Conv2d(in_c, out_channels, kernel_size=1))\n",
    "            \n",
    "        # 3. Create the final fusion convolutions (segmentation head)\n",
    "        # Input channels = num_stages * out_channels\n",
    "        self.fusion_convs = nn.Sequential(\n",
    "            nn.Conv2d(len(backbone_channels) * out_channels, out_channels, \n",
    "                      kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, # Keep the same channel dim\n",
    "                      kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # The diagram shows one more 1x1 conv in the head, let's add it\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 1. Get feature maps from ResNet stages\n",
    "        # This will be a list of tensors, e.g., [C1, C2, C3, C4]\n",
    "        # C1 is 1/4 res, C2 is 1/8, C3 is 1/16, C4 is 1/32\n",
    "        features = self.backbone(x)\n",
    "        \n",
    "        processed_features = []\n",
    "        target_size = features[0].shape[2:] # Size of the C1 feature map (1/4 res)\n",
    "\n",
    "        # 2. Apply 1x1 convs and upsample\n",
    "        for i, feature_map in enumerate(features):\n",
    "            lateral_feature = self.lateral_convs[i](feature_map)\n",
    "            \n",
    "            # Upsample if not the first (highest-res) feature map\n",
    "            if i > 0:\n",
    "                lateral_feature = F.interpolate(lateral_feature, size=target_size, \n",
    "                                                mode='bilinear', align_corners=False)\n",
    "            processed_features.append(lateral_feature)\n",
    "            \n",
    "        # 3. Concatenate all processed & upsampled features\n",
    "        fused_features = torch.cat(processed_features, dim=1)\n",
    "        \n",
    "        # 4. Apply final fusion convolutions\n",
    "        output_feature_map = self.fusion_convs(fused_features)\n",
    "        \n",
    "        return output_feature_map # Shape: (B, out_channels, H/4, W/4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc79da23",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BAM(nn.Module):\n",
    "    \"\"\" Implements the Basic Attention Module (BAM) from diagram (c). \"\"\"\n",
    "    def __init__(self, in_channels):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        \n",
    "        # 1x1 convolutions to generate Query, Key, Value\n",
    "        self.query_conv = nn.Conv2d(in_channels, in_channels // 8, kernel_size=1)\n",
    "        self.key_conv = nn.Conv2d(in_channels, in_channels // 8, kernel_size=1)\n",
    "        self.value_conv = nn.Conv2d(in_channels, in_channels, kernel_size=1)\n",
    "        \n",
    "        self.softmax = nn.Softmax(dim=-1) # Apply softmax along the key dimension\n",
    "\n",
    "        # Learnable scaling parameter\n",
    "        self.gamma = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, C, H, W = x.size()\n",
    "        \n",
    "        # Generate Q, K, V\n",
    "        # Reshape for matrix multiplication: (B, C', N) where N = H*W\n",
    "        proj_query = self.query_conv(x).view(batch_size, -1, W * H).permute(0, 2, 1) # B, N, C'\n",
    "        proj_key = self.key_conv(x).view(batch_size, -1, W * H) # B, C', N\n",
    "        proj_value = self.value_conv(x).view(batch_size, -1, W * H) # B, C, N\n",
    "        \n",
    "        # Calculate attention map: A = softmax(Q @ K)\n",
    "        energy = torch.bmm(proj_query, proj_key) # B, N, N\n",
    "        attention = self.softmax(energy) # B, N, N\n",
    "        \n",
    "        # Apply attention to Value: out = V @ A.T\n",
    "        out = torch.bmm(proj_value, attention.permute(0, 2, 1)) # B, C, N\n",
    "        \n",
    "        # Reshape back to image format: (B, C, H, W)\n",
    "        out = out.view(batch_size, C, H, W)\n",
    "        \n",
    "        # Apply learnable scaling and add residual connection\n",
    "        out = self.gamma * out + x\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "553a5a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PAM(nn.Module):\n",
    "    \"\"\" Implements the Pyramid Attention Module (PAM) from diagram (d). \"\"\"\n",
    "    def __init__(self, in_channels):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Define scales for the pyramid (e.g., 1x1, 2x2, 3x3, 6x6 avg pooling)\n",
    "        self.scales = [1, 2, 3, 6] \n",
    "        \n",
    "        # Create BAM modules for each scale's processing path\n",
    "        self.bams = nn.ModuleList()\n",
    "        # Create 1x1 convs to reduce channels before BAM at different scales\n",
    "        self.convs = nn.ModuleList()\n",
    "        \n",
    "        for _ in self.scales:\n",
    "            # Reduce channels slightly before BAM? (Optional, helps efficiency)\n",
    "            # Or keep the same channels as input\n",
    "            processed_channels = in_channels # Keep same for simplicity\n",
    "            self.convs.append(nn.Sequential(\n",
    "                nn.Conv2d(in_channels, processed_channels, kernel_size=1, bias=False),\n",
    "                nn.BatchNorm2d(processed_channels),\n",
    "                nn.ReLU(inplace=True)\n",
    "            ))\n",
    "            self.bams.append(BAM(processed_channels))\n",
    "            \n",
    "        # Final fusion layer after concatenating pyramid outputs\n",
    "        self.fuse_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels + len(self.scales) * in_channels, in_channels, \n",
    "                      kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(in_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, C, H, W = x.size()\n",
    "        \n",
    "        pyramid_features = [x] # Start with the original features\n",
    "        \n",
    "        for i, scale in enumerate(self.scales):\n",
    "            # Apply pooling for different scales\n",
    "            if scale > 1:\n",
    "                pooled_x = F.adaptive_avg_pool2d(x, (H // scale, W // scale))\n",
    "            else:\n",
    "                pooled_x = x # Scale 1 is the original\n",
    "\n",
    "            # Process with 1x1 conv and BAM\n",
    "            processed_x = self.convs[i](pooled_x)\n",
    "            attended_x = self.bams[i](processed_x)\n",
    "            \n",
    "            # Upsample back to original size\n",
    "            upsampled_x = F.interpolate(attended_x, size=(H, W), mode='bilinear', align_corners=False)\n",
    "            pyramid_features.append(upsampled_x)\n",
    "            \n",
    "        # Concatenate original features and all pyramid features\n",
    "        fused_features = torch.cat(pyramid_features, dim=1)\n",
    "        \n",
    "        # Apply final fusion convolution\n",
    "        out = self.fuse_conv(fused_features)\n",
    "        \n",
    "        # Add residual connection\n",
    "        return out + x # The diagram shows a final residual connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "425eb29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class STANet(nn.Module):\n",
    "    def __init__(self, backbone_name='resnet34', fpn_out_channels=256, \n",
    "                 pam_channels=256, classes=1, pretrained=True, in_channels=3):\n",
    "        super().__init__()\n",
    "        \n",
    "        # --- 1. Siamese Feature Extractor ---\n",
    "        # Create ONE shared instance\n",
    "        self.feature_extractor = ResNetFPNFeatureExtractor(\n",
    "            backbone_name=backbone_name,\n",
    "            pretrained=pretrained,\n",
    "            out_channels=fpn_out_channels,\n",
    "            in_channels=in_channels\n",
    "        )\n",
    "        \n",
    "        # --- 2. Spatial-Temporal Attention Module ---\n",
    "        # Using PAM here as it's more advanced than BAM alone\n",
    "        # Input channels should match the output of the feature extractor\n",
    "        self.pam = PAM(in_channels=fpn_out_channels) \n",
    "        \n",
    "        # --- 3. Metric Module (Final Classifier Head) ---\n",
    "        # Takes the PAM output and produces the final change map\n",
    "        self.metric_module = nn.Sequential(\n",
    "            nn.Conv2d(pam_channels, pam_channels // 4, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(pam_channels // 4),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.1), # Optional dropout\n",
    "            nn.Conv2d(pam_channels // 4, classes, kernel_size=1)\n",
    "            # Final activation (Sigmoid for binary change) will be applied outside or in loss\n",
    "        )\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        # --- 1. Siamese Feature Extraction ---\n",
    "        # Pass both images through the SHARED feature extractor\n",
    "        feat_t1 = self.feature_extractor(x1) # X(1) in diagram\n",
    "        feat_t2 = self.feature_extractor(x2) # X(2) in diagram\n",
    "        \n",
    "        # --- 2. Spatial-Temporal Attention ---\n",
    "        # Apply PAM to both feature maps separately? Or concatenated?\n",
    "        # The diagram seems to show applying attention *after* some implicit fusion?\n",
    "        # Let's assume the standard way: apply attention to refine each map\n",
    "        # *then* compare. OR, apply PAM to the *difference* or *concatenation*.\n",
    "        # The diagram shows PAM applied to *both* X(1) and X(2) leading to Z(1) and Z(2).\n",
    "        \n",
    "        z1 = self.pam(feat_t1) # Z(1) in diagram\n",
    "        z2 = self.pam(feat_t2) # Z(2) in diagram\n",
    "\n",
    "        # --- 3. Final Comparison (Implicit in Metric Module) ---\n",
    "        # How are Z(1) and Z(2) compared? Most likely subtraction or concatenation\n",
    "        # Let's use subtraction as it's common for change detection\n",
    "        feat_diff = torch.abs(z1 - z2)\n",
    "        \n",
    "        # --- 4. Metric Module (Classifier) ---\n",
    "        change_map_logits = self.metric_module(feat_diff)\n",
    "        \n",
    "        # --- 5. Upsample to Original Size ---\n",
    "        # The features are at 1/4 resolution, need to upsample\n",
    "        change_map_logits = F.interpolate(change_map_logits, scale_factor=4, \n",
    "                                          mode='bilinear', align_corners=False)\n",
    "                                          \n",
    "        return change_map_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "983c83fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = STANet(backbone_name='resnet18', pretrained=True, in_channels=3, classes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a672a5f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STANet(\n",
      "  (feature_extractor): ResNetFPNFeatureExtractor(\n",
      "    (backbone): FeatureListNet(\n",
      "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act1): ReLU(inplace=True)\n",
      "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "      (layer1): Sequential(\n",
      "        (0): BasicBlock(\n",
      "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (drop_block): Identity()\n",
      "          (act1): ReLU(inplace=True)\n",
      "          (aa): Identity()\n",
      "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (act2): ReLU(inplace=True)\n",
      "        )\n",
      "        (1): BasicBlock(\n",
      "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (drop_block): Identity()\n",
      "          (act1): ReLU(inplace=True)\n",
      "          (aa): Identity()\n",
      "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (act2): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (layer2): Sequential(\n",
      "        (0): BasicBlock(\n",
      "          (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (drop_block): Identity()\n",
      "          (act1): ReLU(inplace=True)\n",
      "          (aa): Identity()\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (act2): ReLU(inplace=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (1): BasicBlock(\n",
      "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (drop_block): Identity()\n",
      "          (act1): ReLU(inplace=True)\n",
      "          (aa): Identity()\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (act2): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (layer3): Sequential(\n",
      "        (0): BasicBlock(\n",
      "          (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (drop_block): Identity()\n",
      "          (act1): ReLU(inplace=True)\n",
      "          (aa): Identity()\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (act2): ReLU(inplace=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (1): BasicBlock(\n",
      "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (drop_block): Identity()\n",
      "          (act1): ReLU(inplace=True)\n",
      "          (aa): Identity()\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (act2): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (layer4): Sequential(\n",
      "        (0): BasicBlock(\n",
      "          (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (drop_block): Identity()\n",
      "          (act1): ReLU(inplace=True)\n",
      "          (aa): Identity()\n",
      "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (act2): ReLU(inplace=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (1): BasicBlock(\n",
      "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (drop_block): Identity()\n",
      "          (act1): ReLU(inplace=True)\n",
      "          (aa): Identity()\n",
      "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (act2): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (lateral_convs): ModuleList(\n",
      "      (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (fusion_convs): Sequential(\n",
      "      (0): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU(inplace=True)\n",
      "      (6): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "  )\n",
      "  (pam): PAM(\n",
      "    (bams): ModuleList(\n",
      "      (0-3): 4 x BAM(\n",
      "        (query_conv): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (key_conv): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (value_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (softmax): Softmax(dim=-1)\n",
      "      )\n",
      "    )\n",
      "    (convs): ModuleList(\n",
      "      (0-3): 4 x Sequential(\n",
      "        (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (fuse_conv): Sequential(\n",
      "      (0): Conv2d(1280, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (metric_module): Sequential(\n",
      "    (0): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Dropout(p=0.1, inplace=False)\n",
      "    (4): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5d12b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
